lr: 0.001  # Restored to original MDT setting (0.001) for proper convergence
weight_decay: 0.001
epochs: 2300  # ~300k iterations to match original MDT (was 500, need 2282 total)
batch_size: 16
batch_size_val: 1

criterion:
  _target_: utils.losses.LossContainer
  cfg:
    sharp_image:
      weight: 1.0
      loss:
        _target_: utils.losses.CombinedMDTLoss
        l1_weight: 1.0
        fft_weight: 0.1
        reduction: mean

optimizer:
  _target_: torch.optim.AdamW
  lr: ${train.lr}
  weight_decay: ${train.weight_decay}

scheduler:
  _target_: utils.schedulers.CosineAnnealingWarmupLR
  T_max: ${train.epochs}
  eta_min: 0.0000001     # 1e-7 to match original MDT (was 1e-6)
  warmup_epochs: 10      # Linear warmup for first 10 epochs
  warmup_lr: 0.000001    # Start warmup from 1e-6

# Gradient clipping to prevent exploding gradients
grad_clip_norm: 0.5

checkpoint:
  dir: ${hydra:runtime.output_dir}/checkpoints
  patience: 100  # Increased patience for 2300 epochs (was 30)
  min_delta: 0.0001
  monitor: psnr
  mode: max

last_checkpoint: ''
start_epoch: 0  # Set to resume epoch (0-indexed, so 51 means start from epoch 52)
