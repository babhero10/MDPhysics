lr: 0.0002  # Reduced from 0.001 for better stability with transformers + FFT
weight_decay: 0.001
epochs: 500
batch_size: 16
batch_size_val: 1

criterion:
  _target_: utils.losses.LossContainer
  cfg:
    sharp_image:
      weight: 1.0
      loss:
        _target_: utils.losses.CombinedMDTLoss
        l1_weight: 1.0
        fft_weight: 0.1
        reduction: mean

optimizer:
  _target_: torch.optim.AdamW
  lr: ${train.lr}
  weight_decay: ${train.weight_decay}

scheduler:
  _target_: utils.schedulers.CosineAnnealingWarmupLR
  T_max: ${train.epochs}
  eta_min: 0.000001      # 1e-6 minimum learning rate
  warmup_epochs: 10      # Linear warmup for first 10 epochs
  warmup_lr: 0.000001    # Start warmup from 1e-6

# Gradient clipping to prevent exploding gradients
grad_clip_norm: 0.5

checkpoint:
  dir: ${hydra:runtime.output_dir}/checkpoints
  patience: 30  # Increased patience for 300 epochs (was 10)
  min_delta: 0.0001
  monitor: psnr
  mode: max

last_checkpoint: ''
start_epoch: 0  # Set to resume epoch (0-indexed, so 51 means start from epoch 52)
